{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a7291d",
   "metadata": {},
   "source": [
    "# 3.0 Fine-tuning: DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25060393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & global configuration\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    brier_score_loss,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "import optuna\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "SEED = 42\n",
    "VAL_SIZE = 0.10\n",
    "TEST_SIZE = 0.10\n",
    "\n",
    "BASE_CFG = {\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "    \"max_length\": 256,\n",
    "    \"lr\": 2e-5,\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 3,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.06,\n",
    "}\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b29ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "DATASET_IMDB = \"imdb\"\n",
    "DATASET_RT   = \"rotten_tomatoes\"\n",
    "\n",
    "ds_imdb = load_dataset(DATASET_IMDB)\n",
    "ds_rt   = load_dataset(DATASET_RT)\n",
    "\n",
    "def to_df(ds, text_key=\"text\", label_key=\"label\"):\n",
    "    return pd.DataFrame({\"text\": ds[text_key], \"label\": ds[label_key]})\n",
    "\n",
    "# IMDB\n",
    "imdb_train_df = to_df(ds_imdb[\"train\"])\n",
    "imdb_test_df  = to_df(ds_imdb[\"test\"])\n",
    "imdb_full = pd.concat([imdb_train_df, imdb_test_df], ignore_index=True)\n",
    "\n",
    "# Rotten Tomatoes\n",
    "rt_train_df = to_df(ds_rt[\"train\"])\n",
    "rt_val_df   = to_df(ds_rt[\"validation\"])\n",
    "rt_test_df  = to_df(ds_rt[\"test\"])\n",
    "rt_full = pd.concat([rt_train_df, rt_val_df, rt_test_df], ignore_index=True)\n",
    "\n",
    "print(\"IMDB full size:\", len(imdb_full))\n",
    "print(\"RT full size:\", len(rt_full))\n",
    "imdb_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef5bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fixed 80/10/10 stratified splits\n",
    "def stratified_indices(df: pd.DataFrame,\n",
    "                       val_size: float,\n",
    "                       test_size: float,\n",
    "                       seed: int) -> Dict[str, np.ndarray]:\n",
    "    X = df.index.values\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y,\n",
    "        test_size=val_size + test_size,\n",
    "        stratify=y,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    rel_test = test_size / (val_size + test_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=rel_test,\n",
    "        stratify=y_temp,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"train\": np.sort(X_train),\n",
    "        \"val\":   np.sort(X_val),\n",
    "        \"test\":  np.sort(X_test),\n",
    "    }\n",
    "\n",
    "def apply_indices(df: pd.DataFrame,\n",
    "                  idx: Dict[str, np.ndarray]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    tr = df.loc[idx[\"train\"]].reset_index(drop=True)\n",
    "    va = df.loc[idx[\"val\"]].reset_index(drop=True)\n",
    "    te = df.loc[idx[\"test\"]].reset_index(drop=True)\n",
    "    return tr, va, te\n",
    "\n",
    "imdb_idx = stratified_indices(imdb_full, VAL_SIZE, TEST_SIZE, SEED)\n",
    "rt_idx   = stratified_indices(rt_full,   VAL_SIZE, TEST_SIZE, SEED)\n",
    "\n",
    "imdb_tr, imdb_va, imdb_te = apply_indices(imdb_full, imdb_idx)\n",
    "rt_tr,   rt_va,   rt_te   = apply_indices(rt_full,   rt_idx)\n",
    "\n",
    "for name, (tr, va, te) in {\n",
    "    \"IMDB\": (imdb_tr, imdb_va, imdb_te),\n",
    "    \"RT\":   (rt_tr,  rt_va,  rt_te),\n",
    "}.items():\n",
    "    print(f\"{name} splits â€“ train: {len(tr)}, val: {len(va)}, test: {len(te)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5081fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeniser and HF dataset builder\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_CFG[\"model_name\"], use_fast=True)\n",
    "\n",
    "def build_hf(train_df, val_df, test_df, max_length):\n",
    "    \"\"\"\n",
    "    Convert pandas DataFrames into tokenised HF Datasets\n",
    "    suitable for Trainer.\n",
    "    \"\"\"\n",
    "    def tok(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "    def to_hf(df):\n",
    "        return Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "    hf_tr = to_hf(train_df)\n",
    "    hf_va = to_hf(val_df)\n",
    "    hf_te = to_hf(test_df)\n",
    "\n",
    "    hf_tr = hf_tr.map(tok, batched=True)\n",
    "    hf_va = hf_va.map(tok, batched=True)\n",
    "    hf_te = hf_te.map(tok, batched=True)\n",
    "\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"label\"}\n",
    "\n",
    "    def prune(ds):\n",
    "        drop = [c for c in ds.column_names if c not in keep_cols]\n",
    "        return ds.remove_columns(drop)\n",
    "\n",
    "    hf_tr = prune(hf_tr)\n",
    "    hf_va = prune(hf_va)\n",
    "    hf_te = prune(hf_te)\n",
    "\n",
    "    return hf_tr, hf_va, hf_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric  = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    logits = np.array(logits)\n",
    "    logits = logits - logits.max(axis=1, keepdims=True)\n",
    "    probs  = np.exp(logits) / np.exp(logits).sum(axis=1, keepdims=True)\n",
    "    preds  = probs.argmax(axis=1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"macro_f1\": f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "        \"brier\":    brier_score_loss(labels, probs[:, 1]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb18651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainingArguments helper\n",
    "def make_training_args(cfg, save_checkpoints=False):\n",
    "    return TrainingArguments(\n",
    "        output_dir=\"_finetune_tmp\",  \n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\" if save_checkpoints else \"no\",\n",
    "        logging_strategy=\"epoch\",    \n",
    "        learning_rate=float(cfg[\"lr\"]),\n",
    "        per_device_train_batch_size=int(cfg[\"batch_size\"]),\n",
    "        per_device_eval_batch_size=int(cfg[\"batch_size\"]),\n",
    "        num_train_epochs=int(cfg[\"epochs\"]),\n",
    "        weight_decay=float(cfg[\"weight_decay\"]),\n",
    "        warmup_ratio=float(cfg[\"warmup_ratio\"]),\n",
    "        load_best_model_at_end=save_checkpoints,\n",
    "        report_to=[],                 \n",
    "        seed=SEED,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0131ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna: hyperparameter search on IMDB\n",
    "def objective(trial, train_df, val_df, cfg_base):\n",
    "    cfg = dict(cfg_base)\n",
    "    cfg[\"lr\"]           = trial.suggest_loguniform(\"lr\", 1e-5, 5e-4)\n",
    "    cfg[\"batch_size\"]   = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    cfg[\"weight_decay\"] = trial.suggest_loguniform(\"weight_decay\", 1e-4, 1e-1)\n",
    "    cfg[\"warmup_ratio\"] = trial.suggest_uniform(\"warmup_ratio\", 0.0, 0.2)\n",
    "    cfg[\"epochs\"]       = trial.suggest_int(\"epochs\", 2, 4)\n",
    "\n",
    "    hf_tr, hf_va, _ = build_hf(train_df, val_df, val_df, cfg[\"max_length\"])\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg[\"model_name\"],\n",
    "        num_labels=2,\n",
    "    ).to(device)\n",
    "\n",
    "    args = make_training_args(cfg, save_checkpoints=False)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=hf_tr,\n",
    "        eval_dataset=hf_va,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    # Minimise validation loss\n",
    "    return float(eval_results[\"eval_loss\"])\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(\n",
    "    lambda t: objective(t, imdb_tr, imdb_va, BASE_CFG),\n",
    "    n_trials=8,\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "print(\"Best trial:\", study.best_trial.number)\n",
    "print(\"Best value (val loss):\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "\n",
    "CFG_TUNED = dict(BASE_CFG)\n",
    "CFG_TUNED.update(study.best_params)\n",
    "CFG_TUNED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59258bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune and evaluate on one dataset\n",
    "def finetune_and_eval(tag: str,\n",
    "                      train_df: pd.DataFrame,\n",
    "                      val_df: pd.DataFrame,\n",
    "                      test_df: pd.DataFrame,\n",
    "                      cfg: dict,\n",
    "                      n_error_examples: int = 10) -> dict:\n",
    "\n",
    "    hf_tr, hf_va, hf_te = build_hf(train_df, val_df, test_df, cfg[\"max_length\"])\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        cfg[\"model_name\"],\n",
    "        num_labels=2,\n",
    "    ).to(device)\n",
    "\n",
    "    args = make_training_args(cfg, save_checkpoints=False)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=hf_tr,\n",
    "        eval_dataset=hf_va,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluation on test set\n",
    "    preds = trainer.predict(hf_te)\n",
    "    logits = preds.predictions\n",
    "    logits = logits - logits.max(axis=1, keepdims=True)\n",
    "    probs  = np.exp(logits) / np.exp(logits).sum(axis=1, keepdims=True)\n",
    "    y_prob = probs[:, 1]\n",
    "    y_pred = probs.argmax(axis=1)\n",
    "    y_true = np.array(test_df[\"label\"].tolist())\n",
    "\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    f1m   = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    brier = brier_score_loss(y_true, y_prob)\n",
    "\n",
    "    print(f\"\\nTest metrics for {tag}:\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Macro-F1: {f1m:.4f}\")\n",
    "    print(f\"Brier   : {brier:.4f}\")\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=[\"Negative\", \"Positive\"],\n",
    "    ))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"Negative\", \"Positive\"])\n",
    "    disp.plot(values_format=\"d\")\n",
    "    plt.title(f\"Fine-tune: {tag} Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Reliability curve\n",
    "    fracs, means = calibration_curve(y_true, y_prob, n_bins=10, strategy=\"quantile\")\n",
    "    plt.figure()\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\")\n",
    "    plt.plot(means, fracs, marker=\"o\", label=\"Fine-tuned\")\n",
    "    plt.xlabel(\"Predicted probability (bin avg)\")\n",
    "    plt.ylabel(\"Empirical positive rate\")\n",
    "    plt.title(f\"Fine-tune: {tag} Reliability Curve\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Error analysis\n",
    "    print(f\"\\nMisclassified examples for {tag} (up to {n_error_examples}):\\n\")\n",
    "    text_list = test_df[\"text\"].tolist()\n",
    "    errors = []\n",
    "    for t, yt, yp, p in zip(text_list, y_true, y_pred, y_prob):\n",
    "        if yt != yp:\n",
    "            errors.append((t, yt, yp, p))\n",
    "\n",
    "    for i, (t, yt, yp, p) in enumerate(errors[:n_error_examples], 1):\n",
    "        true_lbl = \"Positive\" if yt == 1 else \"Negative\"\n",
    "        pred_lbl = \"Positive\" if yp == 1 else \"Negative\"\n",
    "        print(f\"Example {i}:\")\n",
    "        print(f\"  True label : {true_lbl}\")\n",
    "        print(f\"  Pred label : {pred_lbl}\")\n",
    "        print(f\"  P(Positive): {p:.3f}\")\n",
    "        print(\"  Text:\", t[:400].replace(\"\\n\", \" \"))\n",
    "        if len(t) > 400:\n",
    "            print(\"  ...\")\n",
    "        print()\n",
    "\n",
    "    return {\n",
    "        \"dataset\": tag,\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": f1m,\n",
    "        \"brier\": brier,\n",
    "        \"n_test\": len(y_true),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6faa149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run final fine-tuning\n",
    "metrics = []\n",
    "metrics.append(finetune_and_eval(\"IMDB\", imdb_tr, imdb_va, imdb_te, CFG_TUNED))\n",
    "metrics.append(finetune_and_eval(\"Rotten Tomatoes\", rt_tr, rt_va, rt_te, CFG_TUNED))\n",
    "\n",
    "pd.DataFrame(metrics)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
