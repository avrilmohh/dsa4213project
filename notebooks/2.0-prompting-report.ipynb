{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c86103",
   "metadata": {},
   "source": [
    "# 3.0 Prompting\n",
    "\n",
    "This notebook:\n",
    "- Initializes the Flan-T5-small model and tokenizer for text generation.\n",
    "- Implements zero-shot prompting — model classifies reviews without seeing examples.\n",
    "- Implements few-shot prompting — model is given 4 labeled examples (2 positive, 2 negative) before classification.\n",
    "- Evaluates model predictions using:\n",
    "    - Accuracy, Macro F1, and Brier Score\n",
    "    - Confusion matrices and reliability (calibration) curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bdfdbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, set_seed\n",
    "from sklearn.metrics import accuracy_score, f1_score, brier_score_loss, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "SEED = 20\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "set_seed(SEED)\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "DIR_TABLES = ROOT / \"results\" / \"tables\"\n",
    "DIR_FIGS = ROOT / \"results\" / \"figures\"\n",
    "\n",
    "for p in (DIR_TABLES, DIR_FIGS):\n",
    "    p.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c7a64f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "IMDB full size: 50000\n",
      "RT full size: 10662\n",
      "IMDB test size: 25000\n",
      "RT test size: 1066\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# Load the two datasets\n",
    "DATASET_IMDB = \"imdb\"\n",
    "DATASET_RT = \"rotten_tomatoes\"\n",
    "\n",
    "ds_imdb = load_dataset(DATASET_IMDB)\n",
    "ds_rt = load_dataset(DATASET_RT)\n",
    "\n",
    "def to_df(ds_split, text_key=\"text\", label_key=\"label\"):\n",
    "    return pd.DataFrame({\"text\": ds_split[text_key], \"label\": ds_split[label_key]})\n",
    "\n",
    "# IMDB: train + test concatenated -> full\n",
    "imdb_train_df = to_df(ds_imdb[\"train\"])\n",
    "imdb_test_df = to_df(ds_imdb[\"test\"])\n",
    "imdb_full = pd.concat([imdb_train_df, imdb_test_df], ignore_index=True)\n",
    "\n",
    "# Rotten Tomatoes: train + val + test -> full\n",
    "rt_train_df = to_df(ds_rt[\"train\"])\n",
    "rt_val_df = to_df(ds_rt[\"validation\"])\n",
    "rt_test_df = to_df(ds_rt[\"test\"])\n",
    "rt_full = pd.concat([rt_train_df, rt_val_df, rt_test_df], ignore_index=True)\n",
    "\n",
    "print(\"IMDB full size:\", len(imdb_full))\n",
    "print(\"RT full size:\", len(rt_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b13a8524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "\n",
      "IMDB Few-Shot Ablation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (812 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB Test Metrics with 2 examples: Accuracy=0.930, Macro F1=0.930, Brier Score=0.070\n",
      "IMDB Test Metrics with 4 examples: Accuracy=0.930, Macro F1=0.930, Brier Score=0.070\n",
      "IMDB Test Metrics with 6 examples: Accuracy=0.920, Macro F1=0.920, Brier Score=0.080\n",
      "\n",
      "Rotten Tomatoes Few-Shot Ablation\n",
      "RT Test Metrics with 2 examples: Accuracy=0.800, Macro F1=0.799, Brier Score=0.200\n",
      "RT Test Metrics with 4 examples: Accuracy=0.820, Macro F1=0.820, Brier Score=0.180\n",
      "RT Test Metrics with 6 examples: Accuracy=0.830, Macro F1=0.830, Brier Score=0.170\n",
      "\n",
      "Results saved to c:\\Users\\avril\\Documents\\School\\DSA4213\\Project\\Git\\results\\tables\\few_shot_ablation_results.csv\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on \" + device)\n",
    "model.to(device)\n",
    "seed = 38\n",
    "\n",
    "task_prefix = \"Classify the sentiment of this review as Positive or Negative:\\n\\n\"\n",
    "\n",
    "def zero_shot_predict(texts, max_length=30):\n",
    "    preds = []\n",
    "    for text in texts:\n",
    "        prompt = task_prefix + f\"Review: {text}\\nSentiment:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_length)\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        label = decoded.strip().split()[0].lower().rstrip(\".!,\")\n",
    "        preds.append(1 if \"positive\" in label else 0)\n",
    "    return preds\n",
    "\n",
    "def few_shot_predict(texts, examples, max_length=30):\n",
    "    preds = []\n",
    "    prefix = task_prefix\n",
    "    for ex_text, ex_label in examples:\n",
    "        prefix += f\"Review: {ex_text}\\nSentiment: {ex_label}\\n\\n\"\n",
    "    \n",
    "    for text in texts:\n",
    "        prompt = prefix + f\"Review: {text}\\nSentiment:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_length)\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        label = decoded.strip().split()[0]\n",
    "        preds.append(1 if \"positive\" in label.lower() else 0)\n",
    "    return preds\n",
    "\n",
    "def sample_few_shot_examples(ds_train, n_examples):\n",
    "    np.random.seed(seed)\n",
    "    n_half = n_examples // 2\n",
    "    pos_indices = [i for i, x in enumerate(ds_train) if x[\"label\"] == 1]\n",
    "    neg_indices = [i for i, x in enumerate(ds_train) if x[\"label\"] == 0]\n",
    "    sampled_pos = [int(i) for i in np.random.choice(pos_indices, n_half, replace=False)]\n",
    "    sampled_neg = [int(i) for i in np.random.choice(neg_indices, n_half, replace=False)]\n",
    "    examples = [(ds_train[i][\"text\"], \"Positive\") for i in sampled_pos] + \\\n",
    "               [(ds_train[i][\"text\"], \"Negative\") for i in sampled_neg]\n",
    "    np.random.shuffle(examples)\n",
    "    return examples\n",
    "\n",
    "# Sampled test sets for faster computation\n",
    "sample_size = 100\n",
    "np.random.seed(seed)\n",
    "\n",
    "imdb_sample = imdb_test_df.sample(n=sample_size, random_state=seed)\n",
    "imdb_texts_sample = imdb_sample[\"text\"].tolist()\n",
    "imdb_labels_sample = imdb_sample[\"label\"].tolist()\n",
    "\n",
    "rt_sample = rt_test_df.sample(n=sample_size, random_state=seed)\n",
    "rt_texts_sample = rt_sample[\"text\"].tolist()\n",
    "rt_labels_sample = rt_sample[\"label\"].tolist()\n",
    "\n",
    "example_numbers = [2, 4, 6]  # number of examples from train\n",
    "results_list = []\n",
    "\n",
    "print(\"\\nIMDB Few-Shot Ablation\")\n",
    "for n in example_numbers:\n",
    "    imdb_examples = sample_few_shot_examples(ds_imdb[\"train\"], n)\n",
    "    preds = few_shot_predict(imdb_texts_sample, imdb_examples)\n",
    "    acc = accuracy_score(imdb_labels_sample, preds)\n",
    "    f1 = f1_score(imdb_labels_sample, preds, average=\"macro\")\n",
    "    brier = brier_score_loss(imdb_labels_sample, preds)\n",
    "    print(f\"IMDB Test Metrics with {n} examples: Accuracy={acc:.3f}, Macro F1={f1:.3f}, Brier Score={brier:.3f}\")\n",
    "    results_list.append({\n",
    "        \"dataset\": \"IMDB\",\n",
    "        \"example_count\": n,\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": f1,\n",
    "        \"brier_score\": brier\n",
    "    })\n",
    "\n",
    "print(\"\\nRotten Tomatoes Few-Shot Ablation\")\n",
    "for n in example_numbers:\n",
    "    rt_examples = sample_few_shot_examples(ds_rt[\"train\"], n)\n",
    "    preds = few_shot_predict(rt_texts_sample, rt_examples)\n",
    "    acc = accuracy_score(rt_labels_sample, preds)\n",
    "    f1 = f1_score(rt_labels_sample, preds, average=\"macro\")\n",
    "    brier = brier_score_loss(rt_labels_sample, preds)\n",
    "    print(f\"RT Test Metrics with {n} examples: Accuracy={acc:.3f}, Macro F1={f1:.3f}, Brier Score={brier:.3f}\")\n",
    "    results_list.append({\n",
    "        \"dataset\": \"Rotten Tomatoes\",\n",
    "        \"example_count\": n,\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": f1,\n",
    "        \"brier_score\": brier\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "csv_path = DIR_TABLES / \"few_shot_ablation_results.csv\"\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nResults saved to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea4b9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m imdb_examples_4 \u001b[38;5;241m=\u001b[39m sample_few_shot_examples(ds_imdb[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     10\u001b[0m imdb_zs_preds \u001b[38;5;241m=\u001b[39m zero_shot_predict(imdb_texts)\n\u001b[1;32m---> 11\u001b[0m imdb_fs_preds \u001b[38;5;241m=\u001b[39m \u001b[43mfew_shot_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimdb_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimdb_examples_4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m rt_texts \u001b[38;5;241m=\u001b[39m rt_test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     15\u001b[0m rt_examples_4 \u001b[38;5;241m=\u001b[39m sample_few_shot_examples(ds_rt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m, in \u001b[0;36mfew_shot_predict\u001b[1;34m(texts, examples, max_length)\u001b[0m\n\u001b[0;32m     29\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSentiment:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39mmax_length)\n\u001b[0;32m     32\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     33\u001b[0m label \u001b[38;5;241m=\u001b[39m decoded\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\avril\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\avril\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[1;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m decoding_method(\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2566\u001b[0m     input_ids,\n\u001b[0;32m   2567\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2568\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2569\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_mode_kwargs,\n\u001b[0;32m   2571\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2572\u001b[0m )\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2579\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\avril\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:2787\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2787\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2789\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   2790\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   2791\u001b[0m     outputs,\n\u001b[0;32m   2792\u001b[0m     model_kwargs,\n\u001b[0;32m   2793\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2794\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\avril\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\avril\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\avril\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1764\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1761\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[1;32m-> 1764\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1773\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1774\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1775\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1778\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1780\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\avril\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\avril\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\avril\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1036\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1033\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, mask_seq_length, device\u001b[38;5;241m=\u001b[39minputs_embeds\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_decoder:\n\u001b[1;32m-> 1036\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention_cache\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEncoderDecoderCache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1046\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m attention_mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :]\n",
      "File \u001b[1;32mc:\\Users\\avril\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1209\u001b[0m, in \u001b[0;36mT5Stack._update_causal_mask\u001b[1;34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001b[0m\n\u001b[0;32m   1202\u001b[0m     target_length \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1203\u001b[0m         attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1204\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attention_mask, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m   1205\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m past_seen_tokens \u001b[38;5;241m+\u001b[39m sequence_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1206\u001b[0m     )\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_4d_causal_attention_mask_with_cache_position\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1225\u001b[0m     \u001b[38;5;66;03m# using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\u001b[39;00m\n\u001b[0;32m   1226\u001b[0m     \u001b[38;5;66;03m# Details: https://github.com/pytorch/pytorch/issues/110213\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m     min_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin\n",
      "File \u001b[1;32mc:\\Users\\avril\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1278\u001b[0m, in \u001b[0;36mT5Stack._prepare_4d_causal_attention_mask_with_cache_position\u001b[1;34m(attention_mask, sequence_length, target_length, dtype, cache_position, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m   1276\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m causal_mask\u001b[38;5;241m.\u001b[39mclone()  \u001b[38;5;66;03m# copy to contiguous memory for in-place edit\u001b[39;00m\n\u001b[0;32m   1277\u001b[0m mask_length \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1278\u001b[0m padding_mask \u001b[38;5;241m=\u001b[39m causal_mask[:, :, :, :mask_length] \u001b[38;5;241m+\u001b[39m \u001b[43mattention_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1281\u001b[0m padding_mask \u001b[38;5;241m=\u001b[39m padding_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1282\u001b[0m causal_mask[:, :, :, :mask_length] \u001b[38;5;241m=\u001b[39m causal_mask[:, :, :, :mask_length]\u001b[38;5;241m.\u001b[39mmasked_fill(\n\u001b[0;32m   1283\u001b[0m     padding_mask, min_dtype\n\u001b[0;32m   1284\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on \" + device)\n",
    "model.to(device)\n",
    "\n",
    "# Based on ablation study, giving 4 examples in few shot gives the best results, so we use 4 balanced examples in the final few-shot prompting model\n",
    "# done on full test sets\n",
    "imdb_texts = imdb_test_df[\"text\"].tolist()\n",
    "\n",
    "imdb_examples_4 = sample_few_shot_examples(ds_imdb[\"train\"], 4)\n",
    "imdb_zs_preds = zero_shot_predict(imdb_texts)\n",
    "imdb_fs_preds = few_shot_predict(imdb_texts, imdb_examples_4)\n",
    "\n",
    "rt_texts = rt_test_df[\"text\"].tolist()\n",
    "\n",
    "rt_examples_4 = sample_few_shot_examples(ds_rt[\"train\"], 4)\n",
    "rt_zs_preds = zero_shot_predict(rt_texts)\n",
    "rt_fs_preds = few_shot_predict(rt_texts, rt_examples_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fddea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot(true_labels, zs_preds, fs_preds, texts, dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    true_labels: list of ints 0/1\n",
    "    zs_preds/fs_preds: list of ints 0/1\n",
    "    texts: list of strings\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    def compute_metrics(name, preds):\n",
    "        pred_bin = preds\n",
    "        prob_pos = [float(p) for p in preds]\n",
    "\n",
    "        acc = accuracy_score(true_labels, pred_bin)\n",
    "        f1m = f1_score(true_labels, pred_bin, average=\"macro\")\n",
    "        brier = brier_score_loss(true_labels, prob_pos)\n",
    "\n",
    "        print(f\"\\n{name} metrics ({dataset_name}):\")\n",
    "        print(f\"Accuracy: {acc:.3f}, Macro F1: {f1m:.3f}, Brier Score: {brier:.3f}\")\n",
    "        print(classification_report(true_labels, pred_bin, target_names=[\"Negative\",\"Positive\"]))\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(true_labels, pred_bin, labels=[0,1])\n",
    "        disp = ConfusionMatrixDisplay(cm, display_labels=[\"Negative\",\"Positive\"])\n",
    "        disp.plot(values_format=\"d\")\n",
    "        plt.title(f\"{name} Confusion Matrix: {dataset_name}\")\n",
    "        cm_path = DIR_FIGS / f\"{name.lower().replace(' ', '_')}_{dataset_name.lower()}_cm.png\"\n",
    "        plt.tight_layout(); plt.savefig(cm_path, dpi=150); plt.close()\n",
    "\n",
    "        # Reliability curve\n",
    "        fracs, means = calibration_curve(true_labels, prob_pos, n_bins=10, strategy=\"quantile\")\n",
    "        plt.figure()\n",
    "        plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\")\n",
    "        plt.plot(means, fracs, marker=\"o\", label=\"Prompted\")\n",
    "        plt.xlabel(\"Predicted probability (bin avg)\")\n",
    "        plt.ylabel(\"Empirical positive rate\")\n",
    "        plt.title(f\"{name}: {dataset_name} Reliability Curve\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        rel_path = DIR_FIGS / f\"{name.lower().replace(' ', '_')}_{dataset_name.lower()}_reliability.png\"\n",
    "        plt.savefig(rel_path, dpi=150); plt.close()\n",
    "\n",
    "        results.append({\"dataset\": dataset_name, \"model_name\": name, \"acc\": acc, \"macro_f1\": f1m, \"brier\": brier, \"n_test\": len(true_labels)})\n",
    "\n",
    "        print(f\"\\nMisclassified examples for {name} ({dataset_name}) (up to 10 shown):\\n\")\n",
    "        errors = []\n",
    "        for t, yt, yp, p in zip(texts, true_labels, pred_bin, prob_pos):\n",
    "            if yt != yp:\n",
    "                errors.append((t, yt, yp, p))\n",
    "\n",
    "        for i, (t, yt, yp, p) in enumerate(errors[:10], 1):\n",
    "            true_lbl = \"Positive\" if yt == 1 else \"Negative\"\n",
    "            pred_lbl = \"Positive\" if yp == 1 else \"Negative\"\n",
    "            print(f\"Example {i}:\")\n",
    "            print(f\"  True label : {true_lbl}\")\n",
    "            print(f\"  Pred label : {pred_lbl}\")\n",
    "            print(f\"  P(Positive): {p:.3f}\")\n",
    "            print(\"  Text:\", t[:400].replace(\"\\n\", \" \"))\n",
    "            if len(t) > 400:\n",
    "                print(\"  ...\")\n",
    "            print()\n",
    "\n",
    "    compute_metrics(\"Zero-Shot Flan-T5\", zs_preds)\n",
    "    compute_metrics(\"Few-Shot Flan-T5\", fs_preds)\n",
    "\n",
    "    csv_path = DIR_TABLES / \"prompting_metrics.csv\"\n",
    "    df = pd.DataFrame(results)\n",
    "    if csv_path.exists():\n",
    "        df.to_csv(csv_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved prompting metrics to {csv_path}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# IMDB\n",
    "imdb_true_labels = imdb_test_df[\"label\"].tolist()\n",
    "evaluate_and_plot(imdb_true_labels, imdb_zs_preds, imdb_fs_preds, texts=imdb_test_df[\"text\"].tolist(), dataset_name=\"IMDB\")\n",
    "\n",
    "# Rotten Tomatoes\n",
    "rt_true_labels = rt_test_df[\"label\"].tolist()\n",
    "evaluate_and_plot(rt_true_labels, rt_zs_preds, rt_fs_preds, texts=rt_test_df[\"text\"].tolist(), dataset_name=\"Rotten Tomatoes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
