{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c86103",
   "metadata": {},
   "source": [
    "# 3.0 Prompting\n",
    "\n",
    "This notebook:\n",
    "- Initializes the Flan-T5-small model and tokenizer for text generation.\n",
    "- Implements zero-shot prompting — model classifies reviews without seeing examples.\n",
    "- Implements few-shot prompting — model is given 4 labeled examples (2 positive, 2 negative) before classification.\n",
    "- Evaluates model predictions using:\n",
    "    - Accuracy, Macro F1, and Brier Score\n",
    "    - Confusion matrices and reliability (calibration) curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdfdbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, brier_score_loss, classification_report\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134eab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path.cwd().parent\n",
    "CFG_PATH = ROOT / \"configs\" / \"data.yaml\"\n",
    "\n",
    "with open(CFG_PATH, \"r\") as f:\n",
    "    CFG = yaml.safe_load(f)\n",
    "\n",
    "DIR_SPLITS = ROOT / CFG.get(\"output_splits\", \"data/splits\")\n",
    "DIR_TABLES = ROOT / CFG[\"output_tables\"]\n",
    "DIR_FIGS   = ROOT / CFG[\"output_figures\"]\n",
    "DIR_PREDS  = ROOT / CFG[\"output_preds\"]\n",
    "DIR_MODELS = ROOT / CFG[\"output_models\"]\n",
    "\n",
    "for p in (DIR_TABLES, DIR_FIGS, DIR_PREDS, DIR_MODELS, DIR_SPLITS):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a64f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = CFG.get(\"seed\", 20)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# Load the two datasets\n",
    "dataset_imdb = CFG.get(\"dataset_imdb\", \"imdb\")\n",
    "dataset_rt   = CFG.get(\"dataset_rotten_tomatoes\", \"rotten_tomatoes\")\n",
    "\n",
    "ds_imdb = load_dataset(dataset_imdb)\n",
    "ds_rt   = load_dataset(dataset_rt)\n",
    "\n",
    "# Load indices created from the first notebook\n",
    "with open(DIR_SPLITS / \"imdb_indices.json\") as f:\n",
    "    imdb_splits = json.load(f)\n",
    "\n",
    "with open(DIR_SPLITS / \"rt_indices.json\") as f:\n",
    "    rt_splits = json.load(f)\n",
    "\n",
    "# Concatenate full datasets to allow global indexing\n",
    "ds_imdb_full = concatenate_datasets([ds_imdb[\"train\"], ds_imdb[\"test\"]])\n",
    "ds_rt_full   = concatenate_datasets([ds_rt[\"train\"], ds_rt[\"validation\"], ds_rt[\"test\"]])\n",
    "\n",
    "# Function to select rows based on indices\n",
    "def select_rows(ds, indices, text_key=\"text\", label_key=\"label\"):\n",
    "    data = [ds[i] for i in indices]\n",
    "    return pd.DataFrame({\n",
    "        \"text\": [d[text_key] for d in data],\n",
    "        \"label\": [d[label_key] for d in data]\n",
    "    })\n",
    "\n",
    "# Only get test splits since we are only using test splits for prompting\n",
    "imdb_test = select_rows(ds_imdb_full, imdb_splits[\"test\"])\n",
    "rt_test   = select_rows(ds_rt_full, rt_splits[\"test\"])\n",
    "\n",
    "print(\"IMDB test size:\", len(imdb_test))\n",
    "print(\"RT test size:\", len(rt_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on \" + device)\n",
    "model.to(device)\n",
    "seed = 38\n",
    "\n",
    "task_prefix = \"Classify the sentiment of this review as Positive or Negative:\\n\\n\"\n",
    "\n",
    "def zero_shot_predict(texts, max_length=30):\n",
    "    preds = []\n",
    "    for text in texts:\n",
    "        prompt = task_prefix + f\"Review: {text}\\nSentiment:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_length)\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        label = decoded.strip().split()[0].lower().rstrip(\".!,\")\n",
    "        preds.append(1 if \"positive\" in label else 0)\n",
    "    return preds\n",
    "\n",
    "def few_shot_predict(texts, examples, max_length=30):\n",
    "    preds = []\n",
    "    prefix = task_prefix\n",
    "    for ex_text, ex_label in examples:\n",
    "        prefix += f\"Review: {ex_text}\\nSentiment: {ex_label}\\n\\n\"\n",
    "    \n",
    "    for text in texts:\n",
    "        prompt = prefix + f\"Review: {text}\\nSentiment:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_length)\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        label = decoded.strip().split()[0]\n",
    "        preds.append(1 if \"positive\" in label.lower() else 0)\n",
    "    return preds\n",
    "\n",
    "def sample_few_shot_examples(ds_train, n_examples):\n",
    "    np.random.seed(seed)\n",
    "    n_half = n_examples // 2\n",
    "    pos_indices = [i for i, x in enumerate(ds_train) if x[\"label\"] == 1]\n",
    "    neg_indices = [i for i, x in enumerate(ds_train) if x[\"label\"] == 0]\n",
    "    sampled_pos = np.random.choice(pos_indices, n_half, replace=False)\n",
    "    sampled_neg = np.random.choice(neg_indices, n_half, replace=False)\n",
    "    examples = [(ds_train[i][\"text\"], \"Positive\") for i in sampled_pos] + \\\n",
    "               [(ds_train[i][\"text\"], \"Negative\") for i in sampled_neg]\n",
    "    np.random.shuffle(examples)\n",
    "    return examples\n",
    "\n",
    "# Sampled test sets for faster computation\n",
    "sample_size = 100\n",
    "np.random.seed(seed)\n",
    "\n",
    "imdb_sample = imdb_test.sample(n=sample_size, random_state=seed)\n",
    "imdb_texts_sample = imdb_sample[\"text\"].tolist()\n",
    "imdb_labels_sample = imdb_sample[\"label\"].tolist()\n",
    "\n",
    "rt_sample = rt_test.sample(n=sample_size, random_state=seed)\n",
    "rt_texts_sample = rt_sample[\"text\"].tolist()\n",
    "rt_labels_sample = rt_sample[\"label\"].tolist()\n",
    "\n",
    "example_numbers = [2, 4, 6]  # number of examples from train\n",
    "results_list = []\n",
    "\n",
    "print(\"\\nIMDB Few-Shot Ablation\")\n",
    "for n in example_numbers:\n",
    "    imdb_examples = sample_few_shot_examples(ds_imdb[\"train\"], n)\n",
    "    preds = few_shot_predict(imdb_texts_sample, imdb_examples)\n",
    "    acc = accuracy_score(imdb_labels_sample, preds)\n",
    "    f1 = f1_score(imdb_labels_sample, preds, average=\"macro\")\n",
    "    brier = brier_score_loss(imdb_labels_sample, preds)\n",
    "    print(f\"IMDB Test Metrics with {n} examples: Accuracy={acc:.3f}, Macro F1={f1:.3f}, Brier Score={brier:.3f}\")\n",
    "    results_list.append({\n",
    "        \"dataset\": \"IMDB\",\n",
    "        \"example_count\": n,\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": f1,\n",
    "        \"brier_score\": brier\n",
    "    })\n",
    "\n",
    "print(\"\\nRotten Tomatoes Few-Shot Ablation\")\n",
    "for n in example_numbers:\n",
    "    rt_examples = sample_few_shot_examples(ds_rt[\"train\"], n)\n",
    "    preds = few_shot_predict(rt_texts_sample, rt_examples)\n",
    "    acc = accuracy_score(rt_labels_sample, preds)\n",
    "    f1 = f1_score(rt_labels_sample, preds, average=\"macro\")\n",
    "    brier = brier_score_loss(rt_labels_sample, preds)\n",
    "    print(f\"RT Test Metrics with {n} examples: Accuracy={acc:.3f}, Macro F1={f1:.3f}, Brier Score={brier:.3f}\")\n",
    "    results_list.append({\n",
    "        \"dataset\": \"Rotten Tomatoes\",\n",
    "        \"example_count\": n,\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": f1,\n",
    "        \"brier_score\": brier\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "csv_path = DIR_TABLES / \"few_shot_ablation_results.csv\"\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nResults saved to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on \" + device)\n",
    "model.to(device)\n",
    "\n",
    "# Based on ablation study, giving 4 examples in few shot gives the best results, so we use 4 balanced examples in the final few-shot prompting model\n",
    "# done on full test sets\n",
    "imdb_texts = imdb_test[\"text\"].tolist()\n",
    "\n",
    "imdb_examples_4 = sample_few_shot_examples(ds_imdb[\"train\"], 4)\n",
    "imdb_zs_preds = zero_shot_predict(imdb_texts)\n",
    "imdb_fs_preds = few_shot_predict(imdb_texts, imdb_examples_4)\n",
    "\n",
    "rt_texts = rt_test[\"text\"].tolist()\n",
    "\n",
    "rt_examples_4 = sample_few_shot_examples(ds_rt[\"train\"], 4)\n",
    "rt_zs_preds = zero_shot_predict(rt_texts)\n",
    "rt_fs_preds = few_shot_predict(rt_texts, rt_examples_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fddea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot(true_labels, zs_preds, fs_preds, texts, dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    true_labels: list of integers 0 (Negative) / 1 (Positive)\n",
    "    zs_preds/fs_preds: list of integers 0/1\n",
    "    texts: list of the actual review texts (for JSONL output)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    def compute_metrics(name, preds):\n",
    "        pred_bin = preds\n",
    "        prob_pos = [float(p) for p in preds]  # 1.0 if Positive, 0.0 if Negative\n",
    "\n",
    "        acc = accuracy_score(true_labels, pred_bin)\n",
    "        f1m = f1_score(true_labels, pred_bin, average=\"macro\")\n",
    "        brier = brier_score_loss(true_labels, prob_pos)\n",
    "\n",
    "        print(f\"\\n{name} metrics ({dataset_name}):\")\n",
    "        print(f\"Accuracy: {acc:.3f}, Macro F1: {f1m:.3f}, Brier Score: {brier:.3f}\")\n",
    "        print(classification_report(true_labels, pred_bin, target_names=[\"Negative\",\"Positive\"]))\n",
    "\n",
    "        # Save JSONL predictions (with actual review text)\n",
    "        rows = [{\"text\": t, \"label\": int(y), \"pred\": int(h), \"prob_pos\": float(p)}\n",
    "                for t, y, h, p in zip(texts, true_labels, pred_bin, prob_pos)]\n",
    "        out_path = DIR_PREDS / f\"{name.lower().replace('-','_')}_{dataset_name.lower()}_test.jsonl\"\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for r in rows:\n",
    "                f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(true_labels, pred_bin, labels=[0,1])\n",
    "        disp = ConfusionMatrixDisplay(cm, display_labels=[\"Negative\",\"Positive\"])\n",
    "        disp.plot(values_format=\"d\")\n",
    "        plt.title(f\"{name} Confusion Matrix: {dataset_name}\")\n",
    "        cm_path = DIR_FIGS / f\"{name.lower().replace('-','_')}_{dataset_name.lower()}_cm.png\"\n",
    "        plt.tight_layout(); plt.savefig(cm_path, dpi=150); plt.show()\n",
    "\n",
    "        # Reliability curve\n",
    "        fracs, means = calibration_curve(true_labels, prob_pos, n_bins=10, strategy=\"quantile\")\n",
    "        plt.figure(figsize=(4.8,4))\n",
    "        plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "        plt.plot(fracs, means, marker=\"o\")\n",
    "        plt.xlabel(\"Predicted probability (bin avg)\")\n",
    "        plt.ylabel(\"Empirical positive rate\")\n",
    "        plt.title(f\"{name} Reliability Curve: {dataset_name}\")\n",
    "        rel_path = DIR_FIGS / f\"{name.lower().replace('-','_')}_{dataset_name.lower()}_reliability.png\"\n",
    "        plt.tight_layout(); plt.savefig(rel_path, dpi=150); plt.show()\n",
    "\n",
    "        # Append to results summary\n",
    "        results.append({\n",
    "            \"dataset\": dataset_name,\n",
    "            \"model_name\": name,\n",
    "            \"acc\": acc,\n",
    "            \"macro_f1\": f1m,\n",
    "            \"brier\": brier,\n",
    "            \"n_test\": len(true_labels),\n",
    "        })\n",
    "\n",
    "    # Compute metrics for both zero-shot and few-shot\n",
    "    compute_metrics(\"Zero-Shot Flan-T5\", zs_preds)\n",
    "    compute_metrics(\"Few-Shot Flan-T5\", fs_preds)\n",
    "\n",
    "    # Save/append to combined CSV summary\n",
    "    csv_path = DIR_TABLES / \"prompting_metrics.csv\"\n",
    "    file_exists = csv_path.exists()\n",
    "    with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"dataset\", \"model_name\", \"acc\", \"macro_f1\", \"brier\", \"n_test\"])\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# IMDB\n",
    "imdb_true_labels = imdb_test[\"label\"].tolist()\n",
    "evaluate_and_plot(imdb_true_labels, imdb_zs_preds, imdb_fs_preds, texts=imdb_test[\"text\"].tolist(), dataset_name=\"IMDB\")\n",
    "\n",
    "# Rotten Tomatoes\n",
    "rt_true_labels = rt_test[\"label\"].tolist()\n",
    "evaluate_and_plot(rt_true_labels, rt_zs_preds, rt_fs_preds, texts=rt_test[\"text\"].tolist(), dataset_name=\"Rotten Tomatoes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
