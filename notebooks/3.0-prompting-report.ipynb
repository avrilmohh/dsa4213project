{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c86103",
   "metadata": {},
   "source": [
    "# 3.0 Prompting\n",
    "\n",
    "This notebook:\n",
    "- Initializes the Flan-T5-small model and tokenizer for text generation.\n",
    "- Implements zero-shot prompting — model classifies reviews without seeing examples.\n",
    "- Implements few-shot prompting — model is given 4 labeled examples (2 positive, 2 negative) before classification.\n",
    "- Evaluates model predictions using:\n",
    "    - Accuracy, Macro F1, and Brier Score\n",
    "    - Confusion matrices and reliability (calibration) curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1bdfdbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134eab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid external loggers (e.g., W&B)\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# --- Repo root finder (same pattern as your NB1) ---\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"configs\").exists() or (p / \"src\").exists() or (p / \".git\").exists():\n",
    "            return p\n",
    "    return start\n",
    "\n",
    "ROOT = find_repo_root(Path().resolve())\n",
    "CFG = yaml.safe_load(open(ROOT / \"configs\" / \"data.yaml\"))\n",
    "\n",
    "# Directories (consistent with your project)\n",
    "DIR_TABLES = ROOT / CFG.get(\"output_tables\", \"results/tables\")\n",
    "DIR_FIGS   = ROOT / CFG.get(\"output_figures\", \"results/figures\")\n",
    "DIR_PREDS  = ROOT / CFG.get(\"output_preds\", \"results/preds\")\n",
    "DIR_SPLITS = ROOT / CFG.get(\"output_splits\", \"data/splits\")\n",
    "\n",
    "for p in (DIR_TABLES, DIR_FIGS, DIR_PREDS, DIR_SPLITS):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Seed & device (matches other notebooks)\n",
    "seed = int(CFG.get(\"seed\", 42))\n",
    "set_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a64f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "IMDB test size: 5000\n",
      "RT test size: 1067\n"
     ]
    }
   ],
   "source": [
    "def to_df(ds, text_key=\"text\", label_key=\"label\"):\n",
    "    return pd.DataFrame({\"text\": ds[text_key], \"label\": ds[label_key]})\n",
    "\n",
    "# Load raw datasets (cached)\n",
    "ds_imdb = load_dataset(CFG.get(\"dataset_imdb\", \"imdb\"))\n",
    "ds_rt   = load_dataset(CFG.get(\"dataset_rotten_tomatoes\", \"rotten_tomatoes\"))\n",
    "\n",
    "# Recombine official splits into full sets\n",
    "imdb_full = pd.concat([to_df(ds_imdb[\"train\"]), to_df(ds_imdb[\"test\"])], ignore_index=True)\n",
    "rt_full   = pd.concat([to_df(ds_rt[\"train\"]),  to_df(ds_rt[\"validation\"]), to_df(ds_rt[\"test\"])], ignore_index=True)\n",
    "\n",
    "# Load our unified 80/10/10 indices created in NB1\n",
    "imdb_idx = json.load(open(DIR_SPLITS / \"imdb_indices.json\"))\n",
    "rt_idx   = json.load(open(DIR_SPLITS / \"rt_indices.json\"))\n",
    "\n",
    "def split(df, idx):\n",
    "    tr = df.iloc[idx[\"train\"]].reset_index(drop=True)\n",
    "    va = df.iloc[idx[\"val\"]].reset_index(drop=True)\n",
    "    te = df.iloc[idx[\"test\"]].reset_index(drop=True)\n",
    "    return tr, va, te\n",
    "\n",
    "imdb_tr, imdb_va, imdb_te = split(imdb_full, imdb_idx)\n",
    "rt_tr,   rt_va,   rt_te   = split(rt_full,   rt_idx)\n",
    "\n",
    "print(\"IMDB sizes:\", tuple(map(len, (imdb_tr, imdb_va, imdb_te))))\n",
    "print(\"RT sizes  :\", tuple(map(len, (rt_tr, rt_va, rt_te))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd97fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/flan-t5-small\"   # ~77M params; fast, reproducible\n",
    "tokenizer  = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model      = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "LABELS = [\"Negative\", \"Positive\"]\n",
    "label_to_id = {\"Negative\": 0, \"Positive\": 1}\n",
    "\n",
    "# Small sanitizer to keep prompts shortish & clean\n",
    "def clip_text(t, max_chars=600):\n",
    "    t = (t or \"\").replace(\"\\n\", \" \").strip()\n",
    "    return t[:max_chars]\n",
    "\n",
    "def build_fewshot_exemplars(train_df, k_pos=2, k_neg=2, seed=seed):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    pos = train_df[train_df[\"label\"] == 1].sample(n=k_pos, random_state=seed) if (train_df[\"label\"] == 1).sum() >= k_pos else train_df[train_df[\"label\"] == 1]\n",
    "    neg = train_df[train_df[\"label\"] == 0].sample(n=k_neg, random_state=seed) if (train_df[\"label\"] == 0).sum() >= k_neg else train_df[train_df[\"label\"] == 0]\n",
    "    exs = [(clip_text(t), \"Positive\") for t in pos[\"text\"].tolist()] + [(clip_text(t), \"Negative\") for t in neg[\"text\"].tolist()]\n",
    "    # Shuffle for robustness\n",
    "    rng.shuffle(exs)\n",
    "    return exs\n",
    "\n",
    "def decode_label(text: str):\n",
    "    s = (text or \"\").strip().lower()\n",
    "    if s.startswith(\"pos\"): return \"Positive\"\n",
    "    if s.startswith(\"neg\"): return \"Negative\"\n",
    "    if \"positive\" in s: return \"Positive\"\n",
    "    if \"negative\" in s: return \"Negative\"\n",
    "    # fallback\n",
    "    return \"Negative\"\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_labels(texts, prefix=None, max_new_tokens=16, batch_size=8):\n",
    "    \"\"\"Batch generation; returns list[str] labels decoded from first token(s).\"\"\"\n",
    "    outs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_prompts = []\n",
    "        for t in texts[i:i+batch_size]:\n",
    "            if prefix:\n",
    "                prompt = prefix + f\"Review: {clip_text(t)}\\nSentiment:\"\n",
    "            else:\n",
    "                prompt = f\"Classify the sentiment of this review as Positive or Negative:\\n\\n{clip_text(t)}\\n\\nSentiment:\"\n",
    "            batch_prompts.append(prompt)\n",
    "\n",
    "        enc = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        gen = model.generate(**enc, max_new_tokens=max_new_tokens)\n",
    "        decoded = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "        outs.extend([decode_label(d) for d in decoded])\n",
    "    return outs\n",
    "\n",
    "def fewshot_prefix(examples):\n",
    "    header = \"Classify the sentiment of this review as Positive or Negative:\\n\\n\"\n",
    "    body = \"\".join([f\"Review: {clip_text(t)}\\nSentiment: {lab}\\n\\n\" for t, lab in examples])\n",
    "    return header + body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ffe1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs_from_labels(pred_labels):\n",
    "    # simple pseudo-probability: 1.0 if \"Positive\", 0.0 if \"Negative\"\n",
    "    # (generative models don't emit calibrated class probs in .generate();\n",
    "    #  we keep consistency with Brier definition over binary probs)\n",
    "    return np.array([1.0 if p == \"Positive\" else 0.0 for p in pred_labels], dtype=float)\n",
    "\n",
    "def eval_and_save(test_df, zs_preds, fs_preds, tag: str):\n",
    "    y_true = test_df[\"label\"].to_numpy()\n",
    "    zs_prob = probs_from_labels(zs_preds)\n",
    "    fs_prob = probs_from_labels(fs_preds)\n",
    "    zs_hat  = (zs_prob >= 0.5).astype(int)\n",
    "    fs_hat  = (fs_prob >= 0.5).astype(int)\n",
    "\n",
    "    zs_acc = accuracy_score(y_true, zs_hat)\n",
    "    fs_acc = accuracy_score(y_true, fs_hat)\n",
    "    zs_f1  = f1_score(y_true, zs_hat, average=\"macro\")\n",
    "    fs_f1  = f1_score(y_true, fs_hat, average=\"macro\")\n",
    "    zs_br  = brier_score_loss(y_true, zs_prob)\n",
    "    fs_br  = brier_score_loss(y_true, fs_prob)\n",
    "\n",
    "    # Save predictions\n",
    "    for mode, preds, prob in [(\"zs\", zs_preds, zs_prob), (\"fs\", fs_preds, fs_prob)]:\n",
    "        out_jsonl = DIR_PREDS / f\"prompting_{tag}_{mode}_test.jsonl\"\n",
    "        with open(out_jsonl, \"w\") as f:\n",
    "            for t, y, p, l in zip(test_df[\"text\"], y_true, prob, preds):\n",
    "                f.write(json.dumps({\"text\": t, \"label\": int(y), \"prob_pos\": float(p), \"pred_label\": l}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Confusion matrices & reliability\n",
    "    def plot_and_save(y, yhat, yprob, mode):\n",
    "        cm = confusion_matrix(y, yhat, labels=[0,1])\n",
    "        ConfusionMatrixDisplay(cm, display_labels=[\"neg\",\"pos\"]).plot(values_format=\"d\")\n",
    "        plt.title(f\"Prompting: {tag.upper()} — {mode.upper()} Confusion Matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(DIR_FIGS / f\"prompting_{tag}_cm_{mode}.png\", dpi=150); plt.show()\n",
    "\n",
    "        fr, me = calibration_curve(y, yprob, n_bins=10, strategy=\"quantile\")\n",
    "        plt.figure(figsize=(4.8, 4))\n",
    "        plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "        plt.plot(fr, me, marker=\"o\")\n",
    "        plt.xlabel(\"Predicted probability (bin avg)\")\n",
    "        plt.ylabel(\"Empirical positive rate\")\n",
    "        plt.title(f\"Prompting: {tag.upper()} — {mode.upper()} Reliability\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(DIR_FIGS / f\"prompting_{tag}_reliability_{mode}.png\", dpi=150); plt.show()\n",
    "\n",
    "    plot_and_save(y_true, zs_hat, zs_prob, \"zs\")\n",
    "    plot_and_save(y_true, fs_hat, fs_prob, \"fs\")\n",
    "\n",
    "    return {\n",
    "        \"dataset\": tag,\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"zs_acc\": zs_acc, \"zs_macro_f1\": zs_f1, \"zs_brier\": zs_br,\n",
    "        \"fs_acc\": fs_acc, \"fs_macro_f1\": fs_f1, \"fs_brier\": fs_br,\n",
    "        \"n_test\": len(test_df)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481afccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "# IMDB\n",
    "imdb_examples = build_fewshot_exemplars(imdb_tr, k_pos=2, k_neg=2, seed=seed)\n",
    "zs_imdb = generate_labels(imdb_te[\"text\"].tolist(), prefix=None)\n",
    "fs_imdb = generate_labels(imdb_te[\"text\"].tolist(), prefix=fewshot_prefix(imdb_examples))\n",
    "metrics.append(eval_and_save(imdb_te, zs_imdb, fs_imdb, \"imdb\"))\n",
    "\n",
    "# RT\n",
    "rt_examples = build_fewshot_exemplars(rt_tr, k_pos=2, k_neg=2, seed=seed)\n",
    "zs_rt = generate_labels(rt_te[\"text\"].tolist(), prefix=None)\n",
    "fs_rt = generate_labels(rt_te[\"text\"].tolist(), prefix=fewshot_prefix(rt_examples))\n",
    "metrics.append(eval_and_save(rt_te, zs_rt, fs_rt, \"rt\"))\n",
    "\n",
    "# Save consolidated metrics\n",
    "dfm = pd.DataFrame(metrics)\n",
    "dst = DIR_TABLES / \"prompting_metrics.csv\"\n",
    "dfm.to_csv(dst, index=False)\n",
    "display(dfm)\n",
    "print(\"Saved metrics to:\", dst)\n",
    "print(\"Artifacts →\", \"tables:\", DIR_TABLES, \"| figs:\", DIR_FIGS, \"| preds:\", DIR_PREDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
