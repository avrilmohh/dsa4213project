{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c86103",
   "metadata": {},
   "source": [
    "# 3.0 Prompting\n",
    "\n",
    "This notebook:\n",
    "- Initializes the Flan-T5-small model and tokenizer for text generation.\n",
    "- Implements zero-shot prompting — model classifies reviews without seeing examples.\n",
    "- Implements few-shot prompting — model is given 4 labeled examples (2 positive, 2 negative) before classification.\n",
    "- Evaluates model predictions using:\n",
    "    - Accuracy, Macro F1, and Brier Score\n",
    "    - Confusion matrices and reliability (calibration) curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdfdbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, set_seed\n",
    "from sklearn.metrics import accuracy_score, f1_score, brier_score_loss, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "SEED = 20\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "set_seed(SEED)\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "DIR_TABLES = ROOT / \"results\" / \"tables\"\n",
    "DIR_FIGS = ROOT / \"results\" / \"figures\"\n",
    "\n",
    "for p in (DIR_TABLES, DIR_FIGS):\n",
    "    p.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a64f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# Load the two datasets\n",
    "DATASET_IMDB = \"imdb\"\n",
    "DATASET_RT = \"rotten_tomatoes\"\n",
    "\n",
    "ds_imdb = load_dataset(DATASET_IMDB)\n",
    "ds_rt = load_dataset(DATASET_RT)\n",
    "\n",
    "def to_df(ds_split, text_key=\"text\", label_key=\"label\"):\n",
    "    return pd.DataFrame({\"text\": ds_split[text_key], \"label\": ds_split[label_key]})\n",
    "\n",
    "# IMDB: train + test concatenated -> full\n",
    "imdb_train_df = to_df(ds_imdb[\"train\"])\n",
    "imdb_test_df = to_df(ds_imdb[\"test\"])\n",
    "imdb_full = pd.concat([imdb_train_df, imdb_test_df], ignore_index=True)\n",
    "\n",
    "# Rotten Tomatoes: train + val + test -> full\n",
    "rt_train_df = to_df(ds_rt[\"train\"])\n",
    "rt_val_df = to_df(ds_rt[\"validation\"])\n",
    "rt_test_df = to_df(ds_rt[\"test\"])\n",
    "rt_full = pd.concat([rt_train_df, rt_val_df, rt_test_df], ignore_index=True)\n",
    "\n",
    "print(\"IMDB full size:\", len(imdb_full))\n",
    "print(\"RT full size:\", len(rt_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on \" + device)\n",
    "model.to(device)\n",
    "seed = 38\n",
    "\n",
    "task_prefix = \"Classify the sentiment of this review as Positive or Negative:\\n\\n\"\n",
    "\n",
    "def zero_shot_predict(texts, max_length=30):\n",
    "    preds = []\n",
    "    for text in texts:\n",
    "        prompt = task_prefix + f\"Review: {text}\\nSentiment:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_length)\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        label = decoded.strip().split()[0].lower().rstrip(\".!,\")\n",
    "        preds.append(1 if \"positive\" in label else 0)\n",
    "    return preds\n",
    "\n",
    "def few_shot_predict(texts, examples, max_length=30):\n",
    "    preds = []\n",
    "    prefix = task_prefix\n",
    "    for ex_text, ex_label in examples:\n",
    "        prefix += f\"Review: {ex_text}\\nSentiment: {ex_label}\\n\\n\"\n",
    "    \n",
    "    for text in texts:\n",
    "        prompt = prefix + f\"Review: {text}\\nSentiment:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_length)\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        label = decoded.strip().split()[0]\n",
    "        preds.append(1 if \"positive\" in label.lower() else 0)\n",
    "    return preds\n",
    "\n",
    "def sample_few_shot_examples(ds_train, n_examples):\n",
    "    np.random.seed(seed)\n",
    "    n_half = n_examples // 2\n",
    "    pos_indices = [i for i, x in enumerate(ds_train) if x[\"label\"] == 1]\n",
    "    neg_indices = [i for i, x in enumerate(ds_train) if x[\"label\"] == 0]\n",
    "    sampled_pos = [int(i) for i in np.random.choice(pos_indices, n_half, replace=False)]\n",
    "    sampled_neg = [int(i) for i in np.random.choice(neg_indices, n_half, replace=False)]\n",
    "    examples = [(ds_train[i][\"text\"], \"Positive\") for i in sampled_pos] + \\\n",
    "               [(ds_train[i][\"text\"], \"Negative\") for i in sampled_neg]\n",
    "    np.random.shuffle(examples)\n",
    "    return examples\n",
    "\n",
    "# Sampled test sets for faster computation\n",
    "sample_size = 100\n",
    "np.random.seed(seed)\n",
    "\n",
    "imdb_sample = imdb_test_df.sample(n=sample_size, random_state=seed)\n",
    "imdb_texts_sample = imdb_sample[\"text\"].tolist()\n",
    "imdb_labels_sample = imdb_sample[\"label\"].tolist()\n",
    "\n",
    "rt_sample = rt_test_df.sample(n=sample_size, random_state=seed)\n",
    "rt_texts_sample = rt_sample[\"text\"].tolist()\n",
    "rt_labels_sample = rt_sample[\"label\"].tolist()\n",
    "\n",
    "example_numbers = [2, 4, 6]  # number of examples from train\n",
    "results_list = []\n",
    "\n",
    "print(\"\\nIMDB Few-Shot Ablation\")\n",
    "for n in example_numbers:\n",
    "    imdb_examples = sample_few_shot_examples(ds_imdb[\"train\"], n)\n",
    "    preds = few_shot_predict(imdb_texts_sample, imdb_examples)\n",
    "    acc = accuracy_score(imdb_labels_sample, preds)\n",
    "    f1 = f1_score(imdb_labels_sample, preds, average=\"macro\")\n",
    "    brier = brier_score_loss(imdb_labels_sample, preds)\n",
    "    print(f\"IMDB Test Metrics with {n} examples: Accuracy={acc:.3f}, Macro F1={f1:.3f}, Brier Score={brier:.3f}\")\n",
    "    results_list.append({\n",
    "        \"dataset\": \"IMDB\",\n",
    "        \"example_count\": n,\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": f1,\n",
    "        \"brier_score\": brier\n",
    "    })\n",
    "\n",
    "print(\"\\nRotten Tomatoes Few-Shot Ablation\")\n",
    "for n in example_numbers:\n",
    "    rt_examples = sample_few_shot_examples(ds_rt[\"train\"], n)\n",
    "    preds = few_shot_predict(rt_texts_sample, rt_examples)\n",
    "    acc = accuracy_score(rt_labels_sample, preds)\n",
    "    f1 = f1_score(rt_labels_sample, preds, average=\"macro\")\n",
    "    brier = brier_score_loss(rt_labels_sample, preds)\n",
    "    print(f\"RT Test Metrics with {n} examples: Accuracy={acc:.3f}, Macro F1={f1:.3f}, Brier Score={brier:.3f}\")\n",
    "    results_list.append({\n",
    "        \"dataset\": \"Rotten Tomatoes\",\n",
    "        \"example_count\": n,\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": f1,\n",
    "        \"brier_score\": brier\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "csv_path = DIR_TABLES / \"few_shot_ablation_results.csv\"\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\nResults saved to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Running on \" + device)\n",
    "model.to(device)\n",
    "\n",
    "# Based on ablation study, giving 4 examples in few shot gives the best results, so we use 4 balanced examples in the final few-shot prompting model\n",
    "# done on full test sets\n",
    "imdb_texts = imdb_test_df[\"text\"].tolist()\n",
    "\n",
    "imdb_examples_4 = sample_few_shot_examples(ds_imdb[\"train\"], 4)\n",
    "imdb_zs_preds = zero_shot_predict(imdb_texts)\n",
    "imdb_fs_preds = few_shot_predict(imdb_texts, imdb_examples_4)\n",
    "\n",
    "rt_texts = rt_test_df[\"text\"].tolist()\n",
    "\n",
    "rt_examples_4 = sample_few_shot_examples(ds_rt[\"train\"], 4)\n",
    "rt_zs_preds = zero_shot_predict(rt_texts)\n",
    "rt_fs_preds = few_shot_predict(rt_texts, rt_examples_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fddea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot(true_labels, zs_preds, fs_preds, texts, dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    true_labels: list of ints 0/1\n",
    "    zs_preds/fs_preds: list of ints 0/1\n",
    "    texts: list of strings\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    def compute_metrics(name, preds):\n",
    "        pred_bin = preds\n",
    "        prob_pos = [float(p) for p in preds]\n",
    "\n",
    "        acc = accuracy_score(true_labels, pred_bin)\n",
    "        f1m = f1_score(true_labels, pred_bin, average=\"macro\")\n",
    "        brier = brier_score_loss(true_labels, prob_pos)\n",
    "\n",
    "        print(f\"\\n{name} metrics ({dataset_name}):\")\n",
    "        print(f\"Accuracy: {acc:.3f}, Macro F1: {f1m:.3f}, Brier Score: {brier:.3f}\")\n",
    "        print(classification_report(true_labels, pred_bin, target_names=[\"Negative\",\"Positive\"]))\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(true_labels, pred_bin, labels=[0,1])\n",
    "        disp = ConfusionMatrixDisplay(cm, display_labels=[\"Negative\",\"Positive\"])\n",
    "        disp.plot(values_format=\"d\")\n",
    "        plt.title(f\"{name} Confusion Matrix: {dataset_name}\")\n",
    "        cm_path = DIR_FIGS / f\"{name.lower().replace(' ', '_')}_{dataset_name.lower()}_cm.png\"\n",
    "        plt.tight_layout(); plt.savefig(cm_path, dpi=150); plt.close()\n",
    "\n",
    "        # Reliability curve\n",
    "        fracs, means = calibration_curve(true_labels, prob_pos, n_bins=10, strategy=\"quantile\")\n",
    "        plt.figure()\n",
    "        plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\")\n",
    "        plt.plot(means, fracs, marker=\"o\", label=\"Prompted\")\n",
    "        plt.xlabel(\"Predicted probability (bin avg)\")\n",
    "        plt.ylabel(\"Empirical positive rate\")\n",
    "        plt.title(f\"{name}: {dataset_name} Reliability Curve\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        rel_path = DIR_FIGS / f\"{name.lower().replace(' ', '_')}_{dataset_name.lower()}_reliability.png\"\n",
    "        plt.savefig(rel_path, dpi=150); plt.close()\n",
    "\n",
    "        results.append({\"dataset\": dataset_name, \"model_name\": name, \"acc\": acc, \"macro_f1\": f1m, \"brier\": brier, \"n_test\": len(true_labels)})\n",
    "\n",
    "        print(f\"\\nMisclassified examples for {name} ({dataset_name}) (up to 10 shown):\\n\")\n",
    "        errors = []\n",
    "        for t, yt, yp, p in zip(texts, true_labels, pred_bin, prob_pos):\n",
    "            if yt != yp:\n",
    "                errors.append((t, yt, yp, p))\n",
    "\n",
    "        for i, (t, yt, yp, p) in enumerate(errors[:10], 1):\n",
    "            true_lbl = \"Positive\" if yt == 1 else \"Negative\"\n",
    "            pred_lbl = \"Positive\" if yp == 1 else \"Negative\"\n",
    "            print(f\"Example {i}:\")\n",
    "            print(f\"  True label : {true_lbl}\")\n",
    "            print(f\"  Pred label : {pred_lbl}\")\n",
    "            print(f\"  P(Positive): {p:.3f}\")\n",
    "            print(\"  Text:\", t[:400].replace(\"\\n\", \" \"))\n",
    "            if len(t) > 400:\n",
    "                print(\"  ...\")\n",
    "            print()\n",
    "\n",
    "    compute_metrics(\"Zero-Shot Flan-T5\", zs_preds)\n",
    "    compute_metrics(\"Few-Shot Flan-T5\", fs_preds)\n",
    "\n",
    "    csv_path = DIR_TABLES / \"prompting_metrics.csv\"\n",
    "    df = pd.DataFrame(results)\n",
    "    if csv_path.exists():\n",
    "        df.to_csv(csv_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved prompting metrics to {csv_path}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# IMDB\n",
    "imdb_true_labels = imdb_test_df[\"label\"].tolist()\n",
    "evaluate_and_plot(imdb_true_labels, imdb_zs_preds, imdb_fs_preds, texts=imdb_test_df[\"text\"].tolist(), dataset_name=\"IMDB\")\n",
    "\n",
    "# Rotten Tomatoes\n",
    "rt_true_labels = rt_test_df[\"label\"].tolist()\n",
    "evaluate_and_plot(rt_true_labels, rt_zs_preds, rt_fs_preds, texts=rt_test_df[\"text\"].tolist(), dataset_name=\"Rotten Tomatoes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
